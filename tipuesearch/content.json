{
"A note of *A Survey on Transfer Learning*": {
"tags": "Notes",
"text": "A Survey on Transfer Learning<br/>Introduction<br/>Applied range<br/>knowledge transfer  or transfer learning would be desirable when it is expensive or impossible to recollect the needed training data and rebuild the models.<br/>Examples<br/><br/><br/>Web-document classification<br/>To classify a given Web document into several predefined categories. <br/>For example, the labeled examples may be the university webpages. But the newly created website may have different data features or data distributions. <br/><br/><br/>Data which outdate easily <br/>Example: indoor WiFi localization problems. We wish to adapt the localization model trained in one time period (the source domain) for a new time period (the target domain), or to adapt the localization model trained on a mobile device (the source domain) for a new mobile device (the target domain)<br/><br/><br/>The problem of sentiment classification<br/>To adapt a classification model that is trained on some products to help learn classification models for some other products<br/><br/><br/>Overview<br/>Brief History<br/>History<br/><br/><br/>Traditional machine learning algorithms: make predictions on the future data using statistical models that are trained on previously collected training data<br/><br/><br/>Semisupervised classification: much unlabeled data, little labeled data. Assume they are the same.<br/><br/><br/>Transfer Learning<br/><br/><br/>Motivation: People can intelligently apply knowledge learned previously to solve new problems<br/><br/><br/>Fundamental motivation: A NIPS-95 workshop on \u201cLearning to Learn\u201d<br/><br/><br/>Different names: learning to learn, life-long learning, knowledge transfer, inductive transfer, multitask learning, knowledge consolidation, context-sensitive learning, knowledge-based inductive bias, metalearning, and incremental/cumulative learning<br/><br/><br/>New definition: the ability of a system to recognize and apply knowledge and skills learned in previous tasks to novel tasks.<br/><br/>Difference between transfer learning and mutitask learning: transfer learning cares most about the target task, while multitask learning learning all of the source and target tasks simultaneously<br/><br/><br/><br/>Top venues: <br/><br/>data mining: ACM KDD, IEEE ICDM, and PKDD, for example<br/>machine learning: ICML, NIPS, ECML, AAAI, and IJCAI, for example<br/>applications of machine learning and data mining: ACM SIGIR, WWW, and ACL, for example<br/><br/><br/><br/>Notations and Definitions<br/>Definitions of \u201cDomain\u201d and \u201cTask\u201d<br/>A domain $D$ consists of two components: a feature space $\\\\mathcal{X}$ and a marginal probability distribution $P(X)$, where $X = \\\\{x_1, \\\\dots, x_n\\\\} \\\\in \\\\mathcal{X}$.<br/>A task consists of two components: a label space $\\\\mathcal{Y}$ and an objective predictive function $f(\\\\cdot)$.<br/>This survey only consider one source domain $\\\\mathcal{D}_S$, one target domain $\\\\mathcal{D}_T$ and usually $0 \\\\le N_T \\\\ll N_S$.<br/>Unified definition<br/>Transfer Learning: Given a source domain $\\\\mathcal{D}_S$ and learning task $\\\\mathcal{T}_S$, a target domain $\\\\mathcal{D}_T$ and learning task $\\\\mathcal{T}_T$, transfer learning aims to help improve the learning of the target predictive function $f_T(\\\\cdot)$ in $\\\\mathcal{D}_T$ using the knowledge in $\\\\mathcal{D}_S$ and $\\\\mathcal{T}_S$,where $\\\\mathcal{D}_S \\\\neq \\\\mathcal{D}_T$, or $\\\\mathcal{T}_S \\\\neq \\\\mathcal{T}_T$.<br/>A Categorization of Transfer Learning Techniques<br/>Three main research issues:<br/><br/>what to transfer<br/>how to transfer<br/>when to transfer<br/><br/>Three subsettings:<br/><br/>inductive transfer learning<br/>transductive transfer learning<br/>unsupervised transfer learning<br/><br/>inductive transfer learning<br/>The target task is different from the source task, no matter when the source and target domains are the same or not.<br/>Two cases:<br/>1. A lot of labeled data in the source domain are available.<br/>2. No labeled data in the source domain are available.<br/>transductive transfer learning<br/>The source and target tasks are the same, while the source and target domains are different.<br/>Two cases:<br/>1. $\\\\mathcal{X}_S \\\\neq \\\\mathcal{X}_T$.<br/>2. $\\\\mathcal{X}_S = \\\\mathcal{X}_T$ but $P(X_S) \\\\neq P(X_T)$.<br/>unsupervised transfer learning<br/>no labeled data available in both source and target domains in training.<br/>Inductive transfer learning<br/>Inductive transfer learning: Given a source domain $\\\\mathcal{D}_S$ and a learning task $\\\\mathcal{T}_S$, a target domain $\\\\mathcal{D}_T$ and a learning task $\\\\mathcal{T}_T$, inductive transfer learning aims to help improve the learning of the target predictive function $f_T(\\\\cdot)$ in $\\\\mathcal{D}_T$ using the knowledge in $\\\\mathcal{D}_S$ and $\\\\mathcal{T}_S$ where $\\\\mathcal{T}_S \\\\neq \\\\mathcal{T}_T$.<br/>Transferring Knowledge of Instances<br/>a boosting algorithm: TrAdaBoost<br/>Transferring Knowledge of Feature Representations<br/>Aims at finding \u201cgood\u201d feature representations to minimize domain divergence and classi- fication or regression model error. <br/>Similar to common feature learning in the field of multitask learning<br/>Supervised Feature Construction<br/>In the inductive transfer learning setting, the common features can be learned by solving an optimization problem, given as follows:<br/>$$ argmin_{A, U} \\\\sum_{t \\\\in \\\\{T, S\\\\}} \\\\sum_{i=1}^{n_t} L(y_{t_i}, \\\\langle a_t, U^T x_{t_i}\\\\rangle) + \\\\gamma ||A||^2_{2, 1}$$ <br/>$$s.t. U \\\\in \\\\mathbf{O}^d .$$<br/>Unsupervised Feature Construction<br/>Sparse coding is an unsupervised feature construction method for learning higher level features for transfer learning. It consists of two steps.<br/>In the first step, higher level basis vectors $b = \\\\{b_1, b_2, \\\\dots, b_s\\\\}$ are learned from<br/>$$ min_{a, b}\\\\sum_i ||x_{S_i} - \\\\sum_j a_{S_i}^j b_j||^2_2 + \\\\beta ||a_{S_i}||_1$$<br/>$$s.t. ||b_j||_2 \\\\le 1 , \\\\forall j \\\\in 1, \\\\dots, s. $$<br/>In the second step, higher level features on the target-domain data will be learned based on the basis vectors $b$:<br/>$$a_{T_i}^* = argmin_{a_{T_i}} ||x_{T_i} - \\\\sum_j a_{T_i}^j b_j||^2_2 + \\\\beta||a_{T_i}||_1$$<br/>Transferring Knowledge of Parameters<br/>Most approaches are designed to work under multitask learning. But they can be easily modified for transfer learning. Intuitively, we may assign a larger weight to the loss function of the target domain to achieve better performance in the target domain.<br/>Algorithms: <br/><br/>MT-IVM (Lawrence and Platt)<br/>SVMs for multitask learning (Evgeniou and Pontil)<br/>A locally weighted ensemble learning framework (Gao et al)<br/><br/>Transferring Relational Knowledge<br/>It deals with transfer learning problems in relational domains. It does not assume that the data drawn from each domain be independent and identically distributed (i.i.d.).<br/>Statistical relational learning techniques are proposed to solve these problems.<br/>Algorithms: <br/><br/>TAMAR (Mihalkova et al)<br/><br/>Transductive transfer learning<br/>Transferring the Knowledge of Instances<br/>We want to minimize<br/>$$ \\\\theta^* = argmin_{\\\\theta \\\\in \\\\Theta} \\\\sum_{(x, y)\\\\in D_S} \\\\frac{P(D_T)}{P(D_S)}P(D_S)l(x,y,\\\\theta) \\\\\\\\<br/>\\\\approx argmin_{\\\\theta \\\\in \\\\Theta} \\\\sum_{i=1}^{n_S}\\\\frac{P_T(x_{T_i}, y_{T_i})}{P_S(x_{S_i}, y_{S_i})} l(x_{S_i}, y_{S_i}, \\\\theta).$$<br/>and<br/>$$\\\\frac{P_T(x_{T_i}, y_{T_i})}{P_S(x_{S_i}, y_{S_i})}= \\\\frac{P(x_{S_i})}{P(x_{T_i})}$$<br/>Algorithms to estimate $\\\\frac{P(x_{S_i})}{P(x_{T_i})}$:<br/><br/>kernel-mean matching(KMM\uff09<br/>Kullback-Leibler Importance Estimation Procedure(KLIEP)<br/><br/>Transferring Knowledge of Feature Representations<br/>Structural correspondence learning (SCL) algorithm\uff1a<br/> 1. Define a set of pivot features from both domains<br/> 2. Remove these pivot features from the data and treats each pivot feature as a new label vector. Solve $m$ classification problem:<br/>$$f_1(x)= sgn(w_l^T \\\\cdot x), l=1, \\\\dots, m.$$<br/> 3. Use SVD on matrix $W$: $W = UDV^T$, and $\\\\theta = U^T_{[1:h,:]}$(h is the number of the shared features)is the matrix (linear mapping) whose rows are the top left singular vectors of W.<br/> 4. Use the augmented feature vector to build models.<br/>Disadvantages: How to select the pivot features is difficult and domain dependent.<br/>MI-SCL: Use Mutual Information (MI) to choose the pivot features instead of using more heuristic criteria.<br/>Other algorithms:<br/><br/>coclustering-based algorithm: propagate the label information across different domains.<br/>bridged refinement<br/>spectral classification framework for cross-domain transfer learning problem<br/>a cross-domain text classification algorithm that extended the traditional prob- abilistic latent semantic analysis (PLSA) algorithm<br/><br/>Transfer learning via dimensionality reduction:<br/>Maximum Mean Discrepancy Embedding(MMDE) can learn a low-dimensional space to reduce the difference of distributions between different domains. Transfer Component Analysis (TCA) overcomes the drawback of computational burden.<br/>Unsupervised Transfer Learning<br/>Little research works on this setting. Self-taught clustering (STC) and transferred discriminative analysis (TDA) algorithms are proposed to transfer clustering and transfer dimensionality reduction problems, respectively.<br/>Transferring Knowledge of Feature Representations<br/>Self-taught clustering is an instance of unsupervised transfer learning, which aims at clustering a small collection of unlabeled data in the target domain with the help of a large amount of unlabeled data in the source domain.<br/>STC: tries to learn a common feature space across domains.<br/>TDA algorithm: solves the transfer dimensionality reduction problem. First applies clustering methods to generate pseudoclass labels for the target unlabeled data, then applies dimensionality reduction methods to the target data and labeled source data to reduce the dimensions. Run iteratively.<br/>Transfer Bounds and Negative Transfer<br/>Conditional Kolmogorov complexity is used to measure relatedness between tasks and transfer the \u201cright\u201d amount of information. <br/>A novel graph-based method is used for knowledge transfer.<br/>How to avoid negative transfer is a very important issue. If two tasks are too dissimilar, then brute-force transfer may hurt the performance.<br/>Applications of Transfer Learning<br/>Datasets:<br/>- text mining data sets<br/>- Email spam-filtering data set<br/>- WiFi localization over time periods data set<br/>- Sentiment classification data set<br/>Toolboxes:<br/>a MATLAB toolkit for transfer learning. http://multitask.cs.berkeley.edu/<br/>Other applications:<br/>In sequential machine learning<br/>Conclusion<br/>This survey reviewed several current trends of transfer learning. Three different settings of Transfer Learning: inductive transfer learning, transductive transfer learning, and unsupervised transfer learning. <br/>Approaches can be classified into four contexts based on \u201cwhat to transfer\u201d in learning.<br/>Future research issues:<br/>- how to avoid negative transfer<br/>- how to make sure that no negative transfer happens<br/>- when an entire domain cannot be used for transfer learning and whether we can still transfer part of the domain for useful learning in the target domain.<br/>Most transfer learning algorithms assumed that the feature spaces between the source and target domains are the same. However, we may wish to transfer knowledge across domains or tasks that have different feature spaces, and transfer from multiple such source domains. This type is heterogeneous transfer learning.<br/>Use transfer learning to solve other challenging applications.",
"url": "blog/180602/ASoTL.html"
},
"A note of <i>A Survey on Transfer Learning</i>": {
"tags": "Notes",
"text": "A Survey on Transfer Learning<br/>Introduction<br/>Applied range<br/>knowledge transfer  or transfer learning would be desirable when it is expensive or impossible to recollect the needed training data and rebuild the models.<br/>Examples<br/><br/><br/>Web-document classification<br/>To classify a given Web document into several predefined categories. <br/>For example, the labeled examples may be the university webpages. But the newly created website may have different data features or data distributions. <br/><br/><br/>Data which outdate easily <br/>Example: indoor WiFi localization problems. We wish to adapt the localization model trained in one time period (the source domain) for a new time period (the target domain), or to adapt the localization model trained on a mobile device (the source domain) for a new mobile device (the target domain)<br/><br/><br/>The problem of sentiment classification<br/>To adapt a classification model that is trained on some products to help learn classification models for some other products<br/><br/><br/>Overview<br/>Brief History<br/>History<br/><br/><br/>Traditional machine learning algorithms: make predictions on the future data using statistical models that are trained on previously collected training data<br/><br/><br/>Semisupervised classification: much unlabeled data, little labeled data. Assume they are the same.<br/><br/><br/>Transfer Learning<br/><br/><br/>Motivation: People can intelligently apply knowledge learned previously to solve new problems<br/><br/><br/>Fundamental motivation: A NIPS-95 workshop on \u201cLearning to Learn\u201d<br/><br/><br/>Different names: learning to learn, life-long learning, knowledge transfer, inductive transfer, multitask learning, knowledge consolidation, context-sensitive learning, knowledge-based inductive bias, metalearning, and incremental/cumulative learning<br/><br/><br/>New definition: the ability of a system to recognize and apply knowledge and skills learned in previous tasks to novel tasks.<br/><br/>Difference between transfer learning and mutitask learning: transfer learning cares most about the target task, while multitask learning learning all of the source and target tasks simultaneously<br/><br/><br/><br/>Top venues: <br/><br/>data mining: ACM KDD, IEEE ICDM, and PKDD, for example<br/>machine learning: ICML, NIPS, ECML, AAAI, and IJCAI, for example<br/>applications of machine learning and data mining: ACM SIGIR, WWW, and ACL, for example<br/><br/><br/><br/>Notations and Definitions<br/>Definitions of \u201cDomain\u201d and \u201cTask\u201d<br/>A domain $D$ consists of two components: a feature space $\\\\mathcal{X}$ and a marginal probability distribution $P(X)$, where $X = \\\\{x_1, \\\\dots, x_n\\\\} \\\\in \\\\mathcal{X}$.<br/>A task consists of two components: a label space $\\\\mathcal{Y}$ and an objective predictive function $f(\\\\cdot)$.<br/>This survey only consider one source domain $\\\\mathcal{D}_S$, one target domain $\\\\mathcal{D}_T$ and usually $0 \\\\le N_T \\\\ll N_S$.<br/>Unified definition<br/>Transfer Learning: Given a source domain $\\\\mathcal{D}_S$ and learning task $\\\\mathcal{T}_S$, a target domain $\\\\mathcal{D}_T$ and learning task $\\\\mathcal{T}_T$, transfer learning aims to help improve the learning of the target predictive function $f_T(\\\\cdot)$ in $\\\\mathcal{D}_T$ using the knowledge in $\\\\mathcal{D}_S$ and $\\\\mathcal{T}_S$,where $\\\\mathcal{D}_S \\\\neq \\\\mathcal{D}_T$, or $\\\\mathcal{T}_S \\\\neq \\\\mathcal{T}_T$.<br/>A Categorization of Transfer Learning Techniques<br/>Three main research issues:<br/><br/>what to transfer<br/>how to transfer<br/>when to transfer<br/><br/>Three subsettings:<br/><br/>inductive transfer learning<br/>transductive transfer learning<br/>unsupervised transfer learning<br/><br/>inductive transfer learning<br/>The target task is different from the source task, no matter when the source and target domains are the same or not.<br/>Two cases:<br/>1. A lot of labeled data in the source domain are available.<br/>2. No labeled data in the source domain are available.<br/>transductive transfer learning<br/>The source and target tasks are the same, while the source and target domains are different.<br/>Two cases:<br/>1. $\\\\mathcal{X}_S \\\\neq \\\\mathcal{X}_T$.<br/>2. $\\\\mathcal{X}_S = \\\\mathcal{X}_T$ but $P(X_S) \\\\neq P(X_T)$.<br/>unsupervised transfer learning<br/>no labeled data available in both source and target domains in training.<br/>Inductive transfer learning<br/>Inductive transfer learning: Given a source domain $\\\\mathcal{D}_S$ and a learning task $\\\\mathcal{T}_S$, a target domain $\\\\mathcal{D}_T$ and a learning task $\\\\mathcal{T}_T$, inductive transfer learning aims to help improve the learning of the target predictive function $f_T(\\\\cdot)$ in $\\\\mathcal{D}_T$ using the knowledge in $\\\\mathcal{D}_S$ and $\\\\mathcal{T}_S$ where $\\\\mathcal{T}_S \\\\neq \\\\mathcal{T}_T$.<br/>Transferring Knowledge of Instances<br/>a boosting algorithm: TrAdaBoost<br/>Transferring Knowledge of Feature Representations<br/>Aims at finding \u201cgood\u201d feature representations to minimize domain divergence and classi- fication or regression model error. <br/>Similar to common feature learning in the field of multitask learning<br/>Supervised Feature Construction<br/>In the inductive transfer learning setting, the common features can be learned by solving an optimization problem, given as follows:<br/>$$ argmin_{A, U} \\\\sum_{t \\\\in \\\\{T, S\\\\}} \\\\sum_{i=1}^{n_t} L(y_{t_i}, \\\\langle a_t, U^T x_{t_i}\\\\rangle) + \\\\gamma ||A||^2_{2, 1}$$ <br/>$$s.t. U \\\\in \\\\mathbf{O}^d .$$<br/>Unsupervised Feature Construction<br/>Sparse coding is an unsupervised feature construction method for learning higher level features for transfer learning. It consists of two steps.<br/>In the first step, higher level basis vectors $b = \\\\{b_1, b_2, \\\\dots, b_s\\\\}$ are learned from<br/>$$ min_{a, b}\\\\sum_i ||x_{S_i} - \\\\sum_j a_{S_i}^j b_j||^2_2 + \\\\beta ||a_{S_i}||_1$$<br/>$$s.t. ||b_j||_2 \\\\le 1 , \\\\forall j \\\\in 1, \\\\dots, s. $$<br/>In the second step, higher level features on the target-domain data will be learned based on the basis vectors $b$:<br/>$$a_{T_i}^* = argmin_{a_{T_i}} ||x_{T_i} - \\\\sum_j a_{T_i}^j b_j||^2_2 + \\\\beta||a_{T_i}||_1$$<br/>Transferring Knowledge of Parameters<br/>Most approaches are designed to work under multitask learning. But they can be easily modified for transfer learning. Intuitively, we may assign a larger weight to the loss function of the target domain to achieve better performance in the target domain.<br/>Algorithms: <br/><br/>MT-IVM (Lawrence and Platt)<br/>SVMs for multitask learning (Evgeniou and Pontil)<br/>A locally weighted ensemble learning framework (Gao et al)<br/><br/>Transferring Relational Knowledge<br/>It deals with transfer learning problems in relational domains. It does not assume that the data drawn from each domain be independent and identically distributed (i.i.d.).<br/>Statistical relational learning techniques are proposed to solve these problems.<br/>Algorithms: <br/><br/>TAMAR (Mihalkova et al)<br/><br/>Transductive transfer learning<br/>Transferring the Knowledge of Instances<br/>We want to minimize<br/>$$ \\\\theta^* = argmin_{\\\\theta \\\\in \\\\Theta} \\\\sum_{(x, y)\\\\in D_S} \\\\frac{P(D_T)}{P(D_S)}P(D_S)l(x,y,\\\\theta) \\\\\\\\<br/>\\\\approx argmin_{\\\\theta \\\\in \\\\Theta} \\\\sum_{i=1}^{n_S}\\\\frac{P_T(x_{T_i}, y_{T_i})}{P_S(x_{S_i}, y_{S_i})} l(x_{S_i}, y_{S_i}, \\\\theta).$$<br/>and<br/>$$\\\\frac{P_T(x_{T_i}, y_{T_i})}{P_S(x_{S_i}, y_{S_i})}= \\\\frac{P(x_{S_i})}{P(x_{T_i})}$$<br/>Algorithms to estimate $\\\\frac{P(x_{S_i})}{P(x_{T_i})}$:<br/><br/>kernel-mean matching(KMM\uff09<br/>Kullback-Leibler Importance Estimation Procedure(KLIEP)<br/><br/>Transferring Knowledge of Feature Representations<br/>Structural correspondence learning (SCL) algorithm\uff1a<br/> 1. Define a set of pivot features from both domains<br/> 2. Remove these pivot features from the data and treats each pivot feature as a new label vector. Solve $m$ classification problem:<br/>$$f_1(x)= sgn(w_l^T \\\\cdot x), l=1, \\\\dots, m.$$<br/> 3. Use SVD on matrix $W$: $W = UDV^T$, and $\\\\theta = U^T_{[1:h,:]}$(h is the number of the shared features)is the matrix (linear mapping) whose rows are the top left singular vectors of W.<br/> 4. Use the augmented feature vector to build models.<br/>Disadvantages: How to select the pivot features is difficult and domain dependent.<br/>MI-SCL: Use Mutual Information (MI) to choose the pivot features instead of using more heuristic criteria.<br/>Other algorithms:<br/><br/>coclustering-based algorithm: propagate the label information across different domains.<br/>bridged refinement<br/>spectral classification framework for cross-domain transfer learning problem<br/>a cross-domain text classification algorithm that extended the traditional prob- abilistic latent semantic analysis (PLSA) algorithm<br/><br/>Transfer learning via dimensionality reduction:<br/>Maximum Mean Discrepancy Embedding(MMDE) can learn a low-dimensional space to reduce the difference of distributions between different domains. Transfer Component Analysis (TCA) overcomes the drawback of computational burden.<br/>Unsupervised Transfer Learning<br/>Little research works on this setting. Self-taught clustering (STC) and transferred discriminative analysis (TDA) algorithms are proposed to transfer clustering and transfer dimensionality reduction problems, respectively.<br/>Transferring Knowledge of Feature Representations<br/>Self-taught clustering is an instance of unsupervised transfer learning, which aims at clustering a small collection of unlabeled data in the target domain with the help of a large amount of unlabeled data in the source domain.<br/>STC: tries to learn a common feature space across domains.<br/>TDA algorithm: solves the transfer dimensionality reduction problem. First applies clustering methods to generate pseudoclass labels for the target unlabeled data, then applies dimensionality reduction methods to the target data and labeled source data to reduce the dimensions. Run iteratively.<br/>Transfer Bounds and Negative Transfer<br/>Conditional Kolmogorov complexity is used to measure relatedness between tasks and transfer the \u201cright\u201d amount of information. <br/>A novel graph-based method is used for knowledge transfer.<br/>How to avoid negative transfer is a very important issue. If two tasks are too dissimilar, then brute-force transfer may hurt the performance.<br/>Applications of Transfer Learning<br/>Datasets:<br/>- text mining data sets<br/>- Email spam-filtering data set<br/>- WiFi localization over time periods data set<br/>- Sentiment classification data set<br/>Toolboxes:<br/>a MATLAB toolkit for transfer learning. http://multitask.cs.berkeley.edu/<br/>Other applications:<br/>In sequential machine learning<br/>Conclusion<br/>This survey reviewed several current trends of transfer learning. Three different settings of Transfer Learning: inductive transfer learning, transductive transfer learning, and unsupervised transfer learning. <br/>Approaches can be classified into four contexts based on \u201cwhat to transfer\u201d in learning.<br/>Future research issues:<br/>- how to avoid negative transfer<br/>- how to make sure that no negative transfer happens<br/>- when an entire domain cannot be used for transfer learning and whether we can still transfer part of the domain for useful learning in the target domain.<br/>Most transfer learning algorithms assumed that the feature spaces between the source and target domains are the same. However, we may wish to transfer knowledge across domains or tasks that have different feature spaces, and transfer from multiple such source domains. This type is heterogeneous transfer learning.<br/>Use transfer learning to solve other challenging applications.",
"url": "blog/180602/ASoTL.html"
},
"A note of A Survey on Transfer Learning": {
"tags": "Notes",
"text": "A Survey on Transfer Learning<br/>Introduction<br/>Applied range<br/>knowledge transfer  or transfer learning would be desirable when it is expensive or impossible to recollect the needed training data and rebuild the models.<br/>Examples<br/><br/><br/>Web-document classification<br/>To classify a given Web document into several predefined categories. <br/>For example, the labeled examples may be the university webpages. But the newly created website may have different data features or data distributions. <br/><br/><br/>Data which outdate easily <br/>Example: indoor WiFi localization problems. We wish to adapt the localization model trained in one time period (the source domain) for a new time period (the target domain), or to adapt the localization model trained on a mobile device (the source domain) for a new mobile device (the target domain)<br/><br/><br/>The problem of sentiment classification<br/>To adapt a classification model that is trained on some products to help learn classification models for some other products<br/><br/><br/>Overview<br/>Brief History<br/>History<br/><br/><br/>Traditional machine learning algorithms: make predictions on the future data using statistical models that are trained on previously collected training data<br/><br/><br/>Semisupervised classification: much unlabeled data, little labeled data. Assume they are the same.<br/><br/><br/>Transfer Learning<br/><br/><br/>Motivation: People can intelligently apply knowledge learned previously to solve new problems<br/><br/><br/>Fundamental motivation: A NIPS-95 workshop on \u201cLearning to Learn\u201d<br/><br/><br/>Different names: learning to learn, life-long learning, knowledge transfer, inductive transfer, multitask learning, knowledge consolidation, context-sensitive learning, knowledge-based inductive bias, metalearning, and incremental/cumulative learning<br/><br/><br/>New definition: the ability of a system to recognize and apply knowledge and skills learned in previous tasks to novel tasks.<br/><br/>Difference between transfer learning and mutitask learning: transfer learning cares most about the target task, while multitask learning learning all of the source and target tasks simultaneously<br/><br/><br/><br/>Top venues: <br/><br/>data mining: ACM KDD, IEEE ICDM, and PKDD, for example<br/>machine learning: ICML, NIPS, ECML, AAAI, and IJCAI, for example<br/>applications of machine learning and data mining: ACM SIGIR, WWW, and ACL, for example<br/><br/><br/><br/>Notations and Definitions<br/>Definitions of \u201cDomain\u201d and \u201cTask\u201d<br/>A domain $D$ consists of two components: a feature space $\\\\mathcal{X}$ and a marginal probability distribution $P(X)$, where $X = \\\\{x_1, \\\\dots, x_n\\\\} \\\\in \\\\mathcal{X}$.<br/>A task consists of two components: a label space $\\\\mathcal{Y}$ and an objective predictive function $f(\\\\cdot)$.<br/>This survey only consider one source domain $\\\\mathcal{D}_S$, one target domain $\\\\mathcal{D}_T$ and usually $0 \\\\le N_T \\\\ll N_S$.<br/>Unified definition<br/>Transfer Learning: Given a source domain $\\\\mathcal{D}_S$ and learning task $\\\\mathcal{T}_S$, a target domain $\\\\mathcal{D}_T$ and learning task $\\\\mathcal{T}_T$, transfer learning aims to help improve the learning of the target predictive function $f_T(\\\\cdot)$ in $\\\\mathcal{D}_T$ using the knowledge in $\\\\mathcal{D}_S$ and $\\\\mathcal{T}_S$,where $\\\\mathcal{D}_S \\\\neq \\\\mathcal{D}_T$, or $\\\\mathcal{T}_S \\\\neq \\\\mathcal{T}_T$.<br/>A Categorization of Transfer Learning Techniques<br/>Three main research issues:<br/><br/>what to transfer<br/>how to transfer<br/>when to transfer<br/><br/>Three subsettings:<br/><br/>inductive transfer learning<br/>transductive transfer learning<br/>unsupervised transfer learning<br/><br/>inductive transfer learning<br/>The target task is different from the source task, no matter when the source and target domains are the same or not.<br/>Two cases:<br/>1. A lot of labeled data in the source domain are available.<br/>2. No labeled data in the source domain are available.<br/>transductive transfer learning<br/>The source and target tasks are the same, while the source and target domains are different.<br/>Two cases:<br/>1. $\\\\mathcal{X}_S \\\\neq \\\\mathcal{X}_T$.<br/>2. $\\\\mathcal{X}_S = \\\\mathcal{X}_T$ but $P(X_S) \\\\neq P(X_T)$.<br/>unsupervised transfer learning<br/>no labeled data available in both source and target domains in training.<br/>Inductive transfer learning<br/>Inductive transfer learning: Given a source domain $\\\\mathcal{D}_S$ and a learning task $\\\\mathcal{T}_S$, a target domain $\\\\mathcal{D}_T$ and a learning task $\\\\mathcal{T}_T$, inductive transfer learning aims to help improve the learning of the target predictive function $f_T(\\\\cdot)$ in $\\\\mathcal{D}_T$ using the knowledge in $\\\\mathcal{D}_S$ and $\\\\mathcal{T}_S$ where $\\\\mathcal{T}_S \\\\neq \\\\mathcal{T}_T$.<br/>Transferring Knowledge of Instances<br/>a boosting algorithm: TrAdaBoost<br/>Transferring Knowledge of Feature Representations<br/>Aims at finding \u201cgood\u201d feature representations to minimize domain divergence and classi- fication or regression model error. <br/>Similar to common feature learning in the field of multitask learning<br/>Supervised Feature Construction<br/>In the inductive transfer learning setting, the common features can be learned by solving an optimization problem, given as follows:<br/>$$ argmin_{A, U} \\\\sum_{t \\\\in \\\\{T, S\\\\}} \\\\sum_{i=1}^{n_t} L(y_{t_i}, \\\\langle a_t, U^T x_{t_i}\\\\rangle) + \\\\gamma ||A||^2_{2, 1}$$ <br/>$$s.t. U \\\\in \\\\mathbf{O}^d .$$<br/>Unsupervised Feature Construction<br/>Sparse coding is an unsupervised feature construction method for learning higher level features for transfer learning. It consists of two steps.<br/>In the first step, higher level basis vectors $b = \\\\{b_1, b_2, \\\\dots, b_s\\\\}$ are learned from<br/>$$ min_{a, b}\\\\sum_i ||x_{S_i} - \\\\sum_j a_{S_i}^j b_j||^2_2 + \\\\beta ||a_{S_i}||_1$$<br/>$$s.t. ||b_j||_2 \\\\le 1 , \\\\forall j \\\\in 1, \\\\dots, s. $$<br/>In the second step, higher level features on the target-domain data will be learned based on the basis vectors $b$:<br/>$$a_{T_i}^* = argmin_{a_{T_i}} ||x_{T_i} - \\\\sum_j a_{T_i}^j b_j||^2_2 + \\\\beta||a_{T_i}||_1$$<br/>Transferring Knowledge of Parameters<br/>Most approaches are designed to work under multitask learning. But they can be easily modified for transfer learning. Intuitively, we may assign a larger weight to the loss function of the target domain to achieve better performance in the target domain.<br/>Algorithms: <br/><br/>MT-IVM (Lawrence and Platt)<br/>SVMs for multitask learning (Evgeniou and Pontil)<br/>A locally weighted ensemble learning framework (Gao et al)<br/><br/>Transferring Relational Knowledge<br/>It deals with transfer learning problems in relational domains. It does not assume that the data drawn from each domain be independent and identically distributed (i.i.d.).<br/>Statistical relational learning techniques are proposed to solve these problems.<br/>Algorithms: <br/><br/>TAMAR (Mihalkova et al)<br/><br/>Transductive transfer learning<br/>Transferring the Knowledge of Instances<br/>We want to minimize<br/>$$ \\\\theta^* = argmin_{\\\\theta \\\\in \\\\Theta} \\\\sum_{(x, y)\\\\in D_S} \\\\frac{P(D_T)}{P(D_S)}P(D_S)l(x,y,\\\\theta) \\\\\\\\<br/>\\\\approx argmin_{\\\\theta \\\\in \\\\Theta} \\\\sum_{i=1}^{n_S}\\\\frac{P_T(x_{T_i}, y_{T_i})}{P_S(x_{S_i}, y_{S_i})} l(x_{S_i}, y_{S_i}, \\\\theta).$$<br/>and<br/>$$\\\\frac{P_T(x_{T_i}, y_{T_i})}{P_S(x_{S_i}, y_{S_i})}= \\\\frac{P(x_{S_i})}{P(x_{T_i})}$$<br/>Algorithms to estimate $\\\\frac{P(x_{S_i})}{P(x_{T_i})}$:<br/><br/>kernel-mean matching(KMM\uff09<br/>Kullback-Leibler Importance Estimation Procedure(KLIEP)<br/><br/>Transferring Knowledge of Feature Representations<br/>Structural correspondence learning (SCL) algorithm\uff1a<br/> 1. Define a set of pivot features from both domains<br/> 2. Remove these pivot features from the data and treats each pivot feature as a new label vector. Solve $m$ classification problem:<br/>$$f_1(x)= sgn(w_l^T \\\\cdot x), l=1, \\\\dots, m.$$<br/> 3. Use SVD on matrix $W$: $W = UDV^T$, and $\\\\theta = U^T_{[1:h,:]}$(h is the number of the shared features)is the matrix (linear mapping) whose rows are the top left singular vectors of W.<br/> 4. Use the augmented feature vector to build models.<br/>Disadvantages: How to select the pivot features is difficult and domain dependent.<br/>MI-SCL: Use Mutual Information (MI) to choose the pivot features instead of using more heuristic criteria.<br/>Other algorithms:<br/><br/>coclustering-based algorithm: propagate the label information across different domains.<br/>bridged refinement<br/>spectral classification framework for cross-domain transfer learning problem<br/>a cross-domain text classification algorithm that extended the traditional prob- abilistic latent semantic analysis (PLSA) algorithm<br/><br/>Transfer learning via dimensionality reduction:<br/>Maximum Mean Discrepancy Embedding(MMDE) can learn a low-dimensional space to reduce the difference of distributions between different domains. Transfer Component Analysis (TCA) overcomes the drawback of computational burden.<br/>Unsupervised Transfer Learning<br/>Little research works on this setting. Self-taught clustering (STC) and transferred discriminative analysis (TDA) algorithms are proposed to transfer clustering and transfer dimensionality reduction problems, respectively.<br/>Transferring Knowledge of Feature Representations<br/>Self-taught clustering is an instance of unsupervised transfer learning, which aims at clustering a small collection of unlabeled data in the target domain with the help of a large amount of unlabeled data in the source domain.<br/>STC: tries to learn a common feature space across domains.<br/>TDA algorithm: solves the transfer dimensionality reduction problem. First applies clustering methods to generate pseudoclass labels for the target unlabeled data, then applies dimensionality reduction methods to the target data and labeled source data to reduce the dimensions. Run iteratively.<br/>Transfer Bounds and Negative Transfer<br/>Conditional Kolmogorov complexity is used to measure relatedness between tasks and transfer the \u201cright\u201d amount of information. <br/>A novel graph-based method is used for knowledge transfer.<br/>How to avoid negative transfer is a very important issue. If two tasks are too dissimilar, then brute-force transfer may hurt the performance.<br/>Applications of Transfer Learning<br/>Datasets:<br/>- text mining data sets<br/>- Email spam-filtering data set<br/>- WiFi localization over time periods data set<br/>- Sentiment classification data set<br/>Toolboxes:<br/>a MATLAB toolkit for transfer learning. http://multitask.cs.berkeley.edu/<br/>Other applications:<br/>In sequential machine learning<br/>Conclusion<br/>This survey reviewed several current trends of transfer learning. Three different settings of Transfer Learning: inductive transfer learning, transductive transfer learning, and unsupervised transfer learning. <br/>Approaches can be classified into four contexts based on \u201cwhat to transfer\u201d in learning.<br/>Future research issues:<br/>- how to avoid negative transfer<br/>- how to make sure that no negative transfer happens<br/>- when an entire domain cannot be used for transfer learning and whether we can still transfer part of the domain for useful learning in the target domain.<br/>Most transfer learning algorithms assumed that the feature spaces between the source and target domains are the same. However, we may wish to transfer knowledge across domains or tasks that have different feature spaces, and transfer from multiple such source domains. This type is heterogeneous transfer learning.<br/>Use transfer learning to solve other challenging applications.",
"url": "blog/180602/ASoTL.html"
},
"A note of\u300aA Survey on Transfer Learning\u300b": {
"tags": "Notes",
"text": "A Survey on Transfer Learning<br/>Introduction<br/>Applied range<br/>knowledge transfer  or transfer learning would be desirable when it is expensive or impossible to recollect the needed training data and rebuild the models.<br/>Examples<br/><br/><br/>Web-document classification<br/>To classify a given Web document into several predefined categories. <br/>For example, the labeled examples may be the university webpages. But the newly created website may have different data features or data distributions. <br/><br/><br/>Data which outdate easily <br/>Example: indoor WiFi localization problems. We wish to adapt the localization model trained in one time period (the source domain) for a new time period (the target domain), or to adapt the localization model trained on a mobile device (the source domain) for a new mobile device (the target domain)<br/><br/><br/>The problem of sentiment classification<br/>To adapt a classification model that is trained on some products to help learn classification models for some other products<br/><br/><br/>Overview<br/>Brief History<br/>History<br/><br/><br/>Traditional machine learning algorithms: make predictions on the future data using statistical models that are trained on previously collected training data<br/><br/><br/>Semisupervised classification: much unlabeled data, little labeled data. Assume they are the same.<br/><br/><br/>Transfer Learning<br/><br/><br/>Motivation: People can intelligently apply knowledge learned previously to solve new problems<br/><br/><br/>Fundamental motivation: A NIPS-95 workshop on \u201cLearning to Learn\u201d<br/><br/><br/>Different names: learning to learn, life-long learning, knowledge transfer, inductive transfer, multitask learning, knowledge consolidation, context-sensitive learning, knowledge-based inductive bias, metalearning, and incremental/cumulative learning<br/><br/><br/>New definition: the ability of a system to recognize and apply knowledge and skills learned in previous tasks to novel tasks.<br/><br/>Difference between transfer learning and mutitask learning: transfer learning cares most about the target task, while multitask learning learning all of the source and target tasks simultaneously<br/><br/><br/><br/>Top venues: <br/><br/>data mining: ACM KDD, IEEE ICDM, and PKDD, for example<br/>machine learning: ICML, NIPS, ECML, AAAI, and IJCAI, for example<br/>applications of machine learning and data mining: ACM SIGIR, WWW, and ACL, for example<br/><br/><br/><br/>Notations and Definitions<br/>Definitions of \u201cDomain\u201d and \u201cTask\u201d<br/>A domain $D$ consists of two components: a feature space $\\\\mathcal{X}$ and a marginal probability distribution $P(X)$, where $X = \\\\{x_1, \\\\dots, x_n\\\\} \\\\in \\\\mathcal{X}$.<br/>A task consists of two components: a label space $\\\\mathcal{Y}$ and an objective predictive function $f(\\\\cdot)$.<br/>This survey only consider one source domain $\\\\mathcal{D}_S$, one target domain $\\\\mathcal{D}_T$ and usually $0 \\\\le N_T \\\\ll N_S$.<br/>Unified definition<br/>Transfer Learning: Given a source domain $\\\\mathcal{D}_S$ and learning task $\\\\mathcal{T}_S$, a target domain $\\\\mathcal{D}_T$ and learning task $\\\\mathcal{T}_T$, transfer learning aims to help improve the learning of the target predictive function $f_T(\\\\cdot)$ in $\\\\mathcal{D}_T$ using the knowledge in $\\\\mathcal{D}_S$ and $\\\\mathcal{T}_S$,where $\\\\mathcal{D}_S \\\\neq \\\\mathcal{D}_T$, or $\\\\mathcal{T}_S \\\\neq \\\\mathcal{T}_T$.<br/>A Categorization of Transfer Learning Techniques<br/>Three main research issues:<br/><br/>what to transfer<br/>how to transfer<br/>when to transfer<br/><br/>Three subsettings:<br/><br/>inductive transfer learning<br/>transductive transfer learning<br/>unsupervised transfer learning<br/><br/>inductive transfer learning<br/>The target task is different from the source task, no matter when the source and target domains are the same or not.<br/>Two cases:<br/>1. A lot of labeled data in the source domain are available.<br/>2. No labeled data in the source domain are available.<br/>transductive transfer learning<br/>The source and target tasks are the same, while the source and target domains are different.<br/>Two cases:<br/>1. $\\\\mathcal{X}_S \\\\neq \\\\mathcal{X}_T$.<br/>2. $\\\\mathcal{X}_S = \\\\mathcal{X}_T$ but $P(X_S) \\\\neq P(X_T)$.<br/>unsupervised transfer learning<br/>no labeled data available in both source and target domains in training.<br/>Inductive transfer learning<br/>Inductive transfer learning: Given a source domain $\\\\mathcal{D}_S$ and a learning task $\\\\mathcal{T}_S$, a target domain $\\\\mathcal{D}_T$ and a learning task $\\\\mathcal{T}_T$, inductive transfer learning aims to help improve the learning of the target predictive function $f_T(\\\\cdot)$ in $\\\\mathcal{D}_T$ using the knowledge in $\\\\mathcal{D}_S$ and $\\\\mathcal{T}_S$ where $\\\\mathcal{T}_S \\\\neq \\\\mathcal{T}_T$.<br/>Transferring Knowledge of Instances<br/>a boosting algorithm: TrAdaBoost<br/>Transferring Knowledge of Feature Representations<br/>Aims at finding \u201cgood\u201d feature representations to minimize domain divergence and classi- fication or regression model error. <br/>Similar to common feature learning in the field of multitask learning<br/>Supervised Feature Construction<br/>In the inductive transfer learning setting, the common features can be learned by solving an optimization problem, given as follows:<br/>$$ argmin_{A, U} \\\\sum_{t \\\\in \\\\{T, S\\\\}} \\\\sum_{i=1}^{n_t} L(y_{t_i}, \\\\langle a_t, U^T x_{t_i}\\\\rangle) + \\\\gamma ||A||^2_{2, 1}$$ <br/>$$s.t. U \\\\in \\\\mathbf{O}^d .$$<br/>Unsupervised Feature Construction<br/>Sparse coding is an unsupervised feature construction method for learning higher level features for transfer learning. It consists of two steps.<br/>In the first step, higher level basis vectors $b = \\\\{b_1, b_2, \\\\dots, b_s\\\\}$ are learned from<br/>$$ min_{a, b}\\\\sum_i ||x_{S_i} - \\\\sum_j a_{S_i}^j b_j||^2_2 + \\\\beta ||a_{S_i}||_1$$<br/>$$s.t. ||b_j||_2 \\\\le 1 , \\\\forall j \\\\in 1, \\\\dots, s. $$<br/>In the second step, higher level features on the target-domain data will be learned based on the basis vectors $b$:<br/>$$a_{T_i}^* = argmin_{a_{T_i}} ||x_{T_i} - \\\\sum_j a_{T_i}^j b_j||^2_2 + \\\\beta||a_{T_i}||_1$$<br/>Transferring Knowledge of Parameters<br/>Most approaches are designed to work under multitask learning. But they can be easily modified for transfer learning. Intuitively, we may assign a larger weight to the loss function of the target domain to achieve better performance in the target domain.<br/>Algorithms: <br/><br/>MT-IVM (Lawrence and Platt)<br/>SVMs for multitask learning (Evgeniou and Pontil)<br/>A locally weighted ensemble learning framework (Gao et al)<br/><br/>Transferring Relational Knowledge<br/>It deals with transfer learning problems in relational domains. It does not assume that the data drawn from each domain be independent and identically distributed (i.i.d.).<br/>Statistical relational learning techniques are proposed to solve these problems.<br/>Algorithms: <br/><br/>TAMAR (Mihalkova et al)<br/><br/>Transductive transfer learning<br/>Transferring the Knowledge of Instances<br/>We want to minimize<br/>$$ \\\\theta^* = argmin_{\\\\theta \\\\in \\\\Theta} \\\\sum_{(x, y)\\\\in D_S} \\\\frac{P(D_T)}{P(D_S)}P(D_S)l(x,y,\\\\theta) \\\\\\\\<br/>\\\\approx argmin_{\\\\theta \\\\in \\\\Theta} \\\\sum_{i=1}^{n_S}\\\\frac{P_T(x_{T_i}, y_{T_i})}{P_S(x_{S_i}, y_{S_i})} l(x_{S_i}, y_{S_i}, \\\\theta).$$<br/>and<br/>$$\\\\frac{P_T(x_{T_i}, y_{T_i})}{P_S(x_{S_i}, y_{S_i})}= \\\\frac{P(x_{S_i})}{P(x_{T_i})}$$<br/>Algorithms to estimate $\\\\frac{P(x_{S_i})}{P(x_{T_i})}$:<br/><br/>kernel-mean matching(KMM\uff09<br/>Kullback-Leibler Importance Estimation Procedure(KLIEP)<br/><br/>Transferring Knowledge of Feature Representations<br/>Structural correspondence learning (SCL) algorithm\uff1a<br/> 1. Define a set of pivot features from both domains<br/> 2. Remove these pivot features from the data and treats each pivot feature as a new label vector. Solve $m$ classification problem:<br/>$$f_1(x)= sgn(w_l^T \\\\cdot x), l=1, \\\\dots, m.$$<br/> 3. Use SVD on matrix $W$: $W = UDV^T$, and $\\\\theta = U^T_{[1:h,:]}$(h is the number of the shared features)is the matrix (linear mapping) whose rows are the top left singular vectors of W.<br/> 4. Use the augmented feature vector to build models.<br/>Disadvantages: How to select the pivot features is difficult and domain dependent.<br/>MI-SCL: Use Mutual Information (MI) to choose the pivot features instead of using more heuristic criteria.<br/>Other algorithms:<br/><br/>coclustering-based algorithm: propagate the label information across different domains.<br/>bridged refinement<br/>spectral classification framework for cross-domain transfer learning problem<br/>a cross-domain text classification algorithm that extended the traditional prob- abilistic latent semantic analysis (PLSA) algorithm<br/><br/>Transfer learning via dimensionality reduction:<br/>Maximum Mean Discrepancy Embedding(MMDE) can learn a low-dimensional space to reduce the difference of distributions between different domains. Transfer Component Analysis (TCA) overcomes the drawback of computational burden.<br/>Unsupervised Transfer Learning<br/>Little research works on this setting. Self-taught clustering (STC) and transferred discriminative analysis (TDA) algorithms are proposed to transfer clustering and transfer dimensionality reduction problems, respectively.<br/>Transferring Knowledge of Feature Representations<br/>Self-taught clustering is an instance of unsupervised transfer learning, which aims at clustering a small collection of unlabeled data in the target domain with the help of a large amount of unlabeled data in the source domain.<br/>STC: tries to learn a common feature space across domains.<br/>TDA algorithm: solves the transfer dimensionality reduction problem. First applies clustering methods to generate pseudoclass labels for the target unlabeled data, then applies dimensionality reduction methods to the target data and labeled source data to reduce the dimensions. Run iteratively.<br/>Transfer Bounds and Negative Transfer<br/>Conditional Kolmogorov complexity is used to measure relatedness between tasks and transfer the \u201cright\u201d amount of information. <br/>A novel graph-based method is used for knowledge transfer.<br/>How to avoid negative transfer is a very important issue. If two tasks are too dissimilar, then brute-force transfer may hurt the performance.<br/>Applications of Transfer Learning<br/>Datasets:<br/>- text mining data sets<br/>- Email spam-filtering data set<br/>- WiFi localization over time periods data set<br/>- Sentiment classification data set<br/>Toolboxes:<br/>a MATLAB toolkit for transfer learning. http://multitask.cs.berkeley.edu/<br/>Other applications:<br/>In sequential machine learning<br/>Conclusion<br/>This survey reviewed several current trends of transfer learning. Three different settings of Transfer Learning: inductive transfer learning, transductive transfer learning, and unsupervised transfer learning. <br/>Approaches can be classified into four contexts based on \u201cwhat to transfer\u201d in learning.<br/>Future research issues:<br/>- how to avoid negative transfer<br/>- how to make sure that no negative transfer happens<br/>- when an entire domain cannot be used for transfer learning and whether we can still transfer part of the domain for useful learning in the target domain.<br/>Most transfer learning algorithms assumed that the feature spaces between the source and target domains are the same. However, we may wish to transfer knowledge across domains or tasks that have different feature spaces, and transfer from multiple such source domains. This type is heterogeneous transfer learning.<br/>Use transfer learning to solve other challenging applications.",
"url": "blog/180602/ASoTL.html"
},
"Home Page": {
"tags": "Home",
"text": "\u6b22\u8fce\u6765\u5230 lan-qing\u2019s site\uff01<br/>\u6700\u8fd1\u66f4\u65b0<br/><br/>Added note A note of A Survey on Transfer Learning<br/>copy from riteme<br/>",
"url": "index.html"
},
"\u5173\u4e8e": {
"tags": "About",
"text": "\u5173\u4e8e<br/><br/>\u4eceriteme\u90a3\u91cc\u590d\u5236\u6765\u7684\u535a\u5ba2<br/>",
"url": "about.html"
},
"\u53cb\u94fe": {
"tags": "Friend Links",
"text": "\u53cb\u60c5\u94fe\u63a5",
"url": "links.html"
},
"\u6240\u6709\u6587\u7ae0": {
"tags": "Posts",
"text": "2018-6<br/><br/>A note of A Survey on Transfer Learning<br/>",
"url": "posts.html"
},
"\u6d4b\u8bd5": {
"tags": "test",
"text": "pagegen.py\u7684\u8bd5\u70bc<br/>\u5e0c\u671bpagegen.py\u80fd\u6b63\u786e\u5de5\u4f5c\u3002  <br/>\u5e38\u89c4Markdown\u6d4b\u8bd5<br/>h1<br/>h2<br/>h3<br/>h4<br/>h5<br/>h6<br/>\u4e0a\u9762\u662f\u516d\u7ea7\u6807\u9898\u3002  <br/><br/>\u4e0a\u9762\u662f\u516d\u7ea7\u6807\u9898\u3002<br/>\u8fd9\u662f\u4e00\u6bb5\u5f15\u7528  <br/><br/>\u5f88\u597d\uff0cinline-code\u548cHello, world\uff1a<br/>1<br/>2<br/>3<br/>4<br/>5<br/>6<br/>7<br/>8<br/>9#include <iostream><br/><br/>using namespace std;<br/><br/>int main(int argc, char *argv[]) {<br/>    cout << \\ Hello, world!\\  << endl;<br/><br/>    return 0;<br/>}<br/><br/><br/>\u522b\u5fd8\u4e86Python\uff1a<br/>1<br/>2<br/>3<br/>4#!/usr/bin/env python3<br/><br/>if __name__ == \\ __main__\\ :<br/>    print(\\ Hello, world!\\ )<br/><br/><br/>1<br/>2Tab\u4e5f\u53ef\u4ee5\u76f4\u63a5\u4ee3\u7801<br/>Yeah!!!<br/><br/><br/>\u91cd\u8981\u7684\u8bdd\u8bf4\u4e09\u904d<br/>\u91cd\u8981\u7684\u8bdd\u8bf4\u4e09\u904d<br/>\u91cd\u8981\u7684\u8bdd\u8bf4\u4e09\u904d<br/>\u4e0b\u5212\u7ebf\u662f\u4ec0\u4e48\u9b3c<br/>\u6253\u8868A\u9898\uff1a<br/><br/><br/><br/>NOI<br/>A<br/>B<br/>A + B<br/><br/><br/><br/><br/>1<br/>1<br/>2<br/>3<br/><br/><br/>2<br/>2<br/>2<br/>4<br/><br/><br/>3<br/>5<br/>5<br/>10<br/><br/><br/>4<br/>3<br/>4<br/>7<br/><br/><br/><br/><br/><br/><br/>\u52171<br/>\u52172<br/>\u52173<br/><br/><br/><br/><br/>233333333333333<br/>23333333<br/>233<br/><br/><br/><br/>\u811a\u6ce81\uff1f  <br/>GFM breaks!<br/>\u5e94\u8be5\u4e0d\u5728\u4e00\u884c!<br/>\u5e94\u8be5\u4e0d\u5728\u4e00\u884c!!<br/>\u5e94\u8be5\u4e0d\u5728\u4e00\u884c!!!!!<br/>deleted<br/>inserted<br/>\u2013smartpants\u2014<br/>\u201ca\u201c\u2018b\u2018\u201cc\u201c\u2018d\u2018\u201ce\u201d\u201c\u2019<br/>Mathjax\u6d4b\u8bd5: $e^{ix} = \\\\cos x + i\\\\sin x$<br/>\u8fd9\u4e2a\u5e94\u8be5\u4e0d\u5f97\u51fa\u95ee\u9898......<br/>$$ \\\\sum_{i = 1}^{\\\\infty} i = - {1 \\\\over 12} \\\\tag{1.1} $$<br/>$$ a^2 + b^2 = c^2 \\\\Rightarrow \\\\triangle ABC\\\\text{\u662f\u76f4\u89d2\u4e09\u89d2\u5f62} \\\\tag{1.2} $$<br/>$$<br/>\\\\begin{aligned}<br/>X_k & = \\\\sum^{n - 1}_{j = 0} x_ke^{-2\\\\pi ijk/n} \\\\\\\\<br/>    & = \\\\sum^{n - 1}_{j = 0} x_kw_n^{-jk}<br/>\\\\end{aligned}<br/>$$<br/>$$<br/>\\\\begin{aligned}<br/>x_k & = \\\\frac1n\\\\sum^{n - 1}_{j = 0} X_ke^{2\\\\pi ijk/n} \\\\\\\\<br/>    & = \\\\frac1n\\\\sum^{n - 1}_{j = 0} X_kw_n^{jk}<br/>\\\\end{aligned}<br/>$$<br/>$$ $$$$ $ $<br/>\u884c\u5185\u516c\u5f0finline-math\u5728\u6b64$ 233 \\\\neq 244 $2333333<br/>\u7279\u6b8a\u8bed\u6cd5\u6d4b\u8bd5<br/>FBI Warning<br/>\u80af\u5b9a\u6709BUG<br/><br/>Markdown in it<br/>STRONG, inline-code.<br/><br/>\u4e2d\u6587<br/>Not Supported\u2026<br/>\u4e71\u641e<br/>233444<br/><br/>233<br/><br/>Goodbye!<br/><br/><br/><br/><br/>\u771f\u7684\u884c\u5417......\u00a0\u21a9<br/><br/><br/>",
"url": "blog/1115/test.html"
}
}