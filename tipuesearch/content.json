{
"A note of <i>A survey on Transfer Learning</i>": {
"tags": "Note",
"text": "A Survey on Transfer Learning<br/>Introduction<br/>Applied range<br/>knowledge transfer  or transfer learning would be desirable when it is expensive or impossible to recollect the needed training data and rebuild the models.<br/>Examples<br/><br/><br/>Web-document classification<br/>To classify a given Web document into several predefined categories. <br/>For example, the labeled examples may be the university webpages. But the newly created website may have different data features or data distributions. <br/><br/><br/>Data which outdate easily <br/>Example: indoor WiFi localization problems. We wish to adapt the localization model trained in one time period (the source domain) for a new time period (the target domain), or to adapt the localization model trained on a mobile device (the source domain) for a new mobile device (the target domain)<br/><br/><br/>The problem of sentiment classification<br/>To adapt a classification model that is trained on some products to help learn classification models for some other products<br/><br/><br/>Overview<br/>Brief History<br/>History<br/><br/><br/>Traditional machine learning algorithms: make predictions on the future data using statistical models that are trained on previously collected training data<br/><br/><br/>Semisupervised classification: much unlabeled data, little labeled data. Assume they are the same.<br/><br/><br/>Transfer Learning<br/><br/><br/>Motivation: People can intelligently apply knowledge learned previously to solve new problems<br/><br/><br/>Fundamental motivation: A NIPS-95 workshop on \u201cLearning to Learn\u201d<br/><br/><br/>Different names: learning to learn, life-long learning, knowledge transfer, inductive transfer, multitask learning, knowledge consolidation, context-sensitive learning, knowledge-based inductive bias, metalearning, and incremental/cumulative learning<br/><br/><br/>New definition: the ability of a system to recognize and apply knowledge and skills learned in previous tasks to novel tasks.<br/><br/>Difference between transfer learning and mutitask learning: transfer learning cares most about the target task, while multitask learning learning all of the source and target tasks simultaneously<br/><br/><br/><br/>Top venues: <br/><br/>data mining: ACM KDD, IEEE ICDM, and PKDD, for example<br/>machine learning: ICML, NIPS, ECML, AAAI, and IJCAI, for example<br/>applications of machine learning and data mining: ACM SIGIR, WWW, and ACL, for example<br/><br/><br/><br/>Notations and Definitions<br/>Definitions of \u201cDomain\u201d and \u201cTask\u201d<br/>A domain $D$ consists of two components: a feature space $\\\\mathcal{X}$ and a marginal probability distribution $P(X)$, where $X = \\\\{x_1, \\\\dots, x_n\\\\} \\\\in \\\\mathcal{X}$.<br/>A task consists of two components: a label space $\\\\mathcal{Y}$ and an objective predictive function $f(\\\\cdot)$.<br/>This survey only consider one source domain $\\\\mathcal{D}_S$, one target domain $\\\\mathcal{D}_T$ and usually $0 \\\\le N_T \\\\ll N_S$.<br/>Unified definition<br/>Transfer Learning: Given a source domain $\\\\mathcal{D}_S$ and learning task $\\\\mathcal{T}_S$, a target domain $\\\\mathcal{D}_T$ and learning task $\\\\mathcal{T}_T$, transfer learning aims to help improve the learning of the target predictive function $f_T(\\\\cdot)$ in $\\\\mathcal{D}_T$ using the knowledge in $\\\\mathcal{D}_S$ and $\\\\mathcal{T}_S$,where $\\\\mathcal{D}_S \\\\neq \\\\mathcal{D}_T$, or $\\\\mathcal{T}_S \\\\neq \\\\mathcal{T}_T$.<br/>A Categorization of Transfer Learning Techniques<br/>Three main research issues:<br/><br/>what to transfer<br/>how to transfer<br/>when to transfer<br/><br/>Three subsettings:<br/><br/>inductive transfer learning<br/>transductive transfer learning<br/>unsupervised transfer learning<br/><br/>inductive transfer learning<br/>The target task is different from the source task, no matter when the source and target domains are the same or not.<br/>Two cases:<br/>1. A lot of labeled data in the source domain are available.<br/>2. No labeled data in the source domain are available.<br/>transductive transfer learning<br/>The source and target tasks are the same, while the source and target domains are different.<br/>Two cases:<br/>1. $\\\\mathcal{X}_S \\\\neq \\\\mathcal{X}_T$.<br/>2. $\\\\mathcal{X}_S = \\\\mathcal{X}_T$ but $P(X_S) \\\\neq P(X_T)$.<br/>unsupervised transfer learning<br/>no labeled data available in both source and target domains in training.<br/>Inductive transfer learning<br/>Inductive transfer learning: Given a source domain $\\\\mathcal{D}_S$ and a learning task $\\\\mathcal{T}_S$, a target domain $\\\\mathcal{D}_T$ and a learning task $\\\\mathcal{T}_T$, inductive transfer learning aims to help improve the learning of the target predictive function $f_T(\\\\cdot)$ in $\\\\mathcal{D}_T$ using the knowledge in $\\\\mathcal{D}_S$ and $\\\\mathcal{T}_S$ where $\\\\mathcal{T}_S \\\\neq \\\\mathcal{T}_T$.<br/>Transferring Knowledge of Instances<br/>a boosting algorithm: TrAdaBoost<br/>Transferring Knowledge of Feature Representations",
"url": "blog/180602/ASoTL.html"
}
}