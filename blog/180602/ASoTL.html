<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>A note of A Survey on Transfer Learning - lan-qing.site</title>
  <link rel="shortcut icon" href="/favicon.png" type="image/png">
  <link rel="stylesheet" href="/material/material-icons.css">
  <link rel="stylesheet" href="/material/material.min.css" >
  <link rel="stylesheet" href="/gitment/gitment.css">
  <link rel="stylesheet" href="/math-renderer/katex/katex.min.css">
  <link rel="stylesheet" href="/css/site.css">
  <script src="/jquery/jquery-2.2.4.min.js"></script>
  <script src="/jquery/js-cookie.js"></script>
  <script defer src="/material/material.min.js"></script>
  <script src="/math-renderer/selector.js"></script>
  <script src="/gitment/gitment.js"></script>
  <script src="/valine/av-min.js"></script>
  <script src="/valine/Valine.min.js"></script>

</head>
<body>
  <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header">
    <header class="mdl-layout__header">
      <div class="mdl-layout__header-row">
        <span class="mdl-layout-title">A note of A Survey on Transfer Learning</span>
        <div class="mdl-layout-spacer"></div>
        <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label class="mdl-button mdl-js-button mdl-button--icon" for="fixed-header-drawer-exp"><i class="material-icons">search</i></label>
        <div class="mdl-textfield__expandable-holder">
          <form action="/search.html"><input type="text" class="mdl-textfield__input" placeholder="Search Here" name="q" id="fixed-header-drawer-exp" autocomplete="off" required></form>
        </div>
      </div>
    </div>
  </header>
  <div class="mdl-layout__drawer">
    <span class="mdl-layout-title drawer-title">
      <a href="/index.html" style="color: inherit; font-weight: inherit;text-decoration: none;">
        <img src="/favicon.png" width=32 height=32> lan-qing.site
      </a>
    </span>
    <nav class="mdl-navigation">
      <a class="mdl-navigation__link" href="/index.html"><i class="material-icons drawer-icon">home</i> 首页</a>
      <a class="mdl-navigation__link" href="/posts.html"><i class="material-icons drawer-icon">library_books</i> 所有文章</a>
      <a class="mdl-navigation__link" href="/about.html"><i class="material-icons drawer-icon">info</i> 关于</a>
      <a class="mdl-navigation__link" href="/search.html"><i class="material-icons drawer-icon">search</i> 搜索</a>
      <a class="mdl-navigation__link" href="https://github.com/lan-qing/lan-qing.github.io"><i class="material-icons drawer-icon">class</i> GitHub 项目</a>
    </nav>
  </div>
  <main class="mdl-layout__content">
    <div class="mdl-grid">
      <div class="mdl-cell mdl-cell--9-col mdl-cell--12-col-tablet mdl-cell--12-col-phone">
        <div class="article main-article" lang="en-US">
          
<h1 id="a-note-of-a-survey-on-transfer-learning">A note of A Survey on Transfer Learning</h1>
<h2 id="introduction">Introduction</h2>
<h4 id="applied-range">Applied range</h4>
<p><em>knowledge transfer</em>  or <em>transfer learning</em> would be desirable when it is expensive or impossible to recollect the needed training data and rebuild the models.</p>
<h4 id="examples">Examples</h4>
<ul>
<li>
<p>Web-document classification</p>
<p>To classify a given Web document into several predefined categories. </p>
<p>For example, the labeled examples may be the university webpages. But the newly created website may have different data features or data distributions. </p>
</li>
<li>
<p>Data which outdate easily </p>
<p>Example: indoor WiFi localization problems. We wish to adapt the localization model trained in one time period (the source domain) for a new time period (the target domain), or to adapt the localization model trained on a mobile device (the source domain) for a new mobile device (the target domain)</p>
</li>
<li>
<p>The problem of sentiment classification</p>
<p>To adapt a classification model that is trained on some products to help learn classification models for some other products</p>
</li>
</ul>
<h2 id="overview">Overview</h2>
<h3 id="brief-history">Brief History</h3>
<h4 id="history">History</h4>
<ul>
<li>
<p>Traditional machine learning algorithms: make predictions on the future data using statistical models that are trained on previously collected training data</p>
</li>
<li>
<p>Semisupervised classification: much unlabeled data, little labeled data. Assume they are the same.</p>
</li>
</ul>
<h4 id="transfer-learning">Transfer Learning</h4>
<ul>
<li>
<p>Motivation: People can intelligently apply knowledge learned previously to solve new problems</p>
</li>
<li>
<p>Fundamental motivation: A NIPS-95 workshop on “Learning to Learn”</p>
</li>
<li>
<p>Different names: learning to learn, life-long learning, knowledge transfer, inductive transfer, multitask learning, knowledge consolidation, context-sensitive learning, knowledge-based inductive bias, metalearning, and incremental/cumulative learning</p>
</li>
<li>
<p>New definition: the ability of a system to recognize and apply knowledge and skills learned in previous tasks to novel tasks.</p>
<ul>
<li>Difference between transfer learning and mutitask learning: transfer learning cares most about the target task, while multitask learning learning all of the source and target tasks simultaneously</li>
</ul>
</li>
<li>
<p>Top venues: </p>
<ul>
<li>data mining: ACM KDD, IEEE ICDM, and PKDD, for example</li>
<li>machine learning: ICML, NIPS, ECML, AAAI, and IJCAI, for example</li>
<li>applications of machine learning and data mining: ACM SIGIR, WWW, and ACL, for example</li>
</ul>
</li>
</ul>
<h3 id="notations-and-definitions">Notations and Definitions</h3>
<h4 id="definitions-of-domain-and-task">Definitions of &ldquo;Domain&rdquo; and &ldquo;Task&rdquo;</h4>
<p>A <em>domain</em> <mathjax>$D$</mathjax> consists of two components: a feature space <mathjax>$\mathcal{X}$</mathjax> and a marginal probability distribution <mathjax>$P(X)$</mathjax>, where <mathjax>$X = \{x_1, \dots, x_n\} \in \mathcal{X}$</mathjax>.</p>
<p>A <em>task</em> consists of two components: a label space <mathjax>$\mathcal{Y}$</mathjax> and an objective predictive function <mathjax>$f(\cdot)$</mathjax>.</p>
<p>This survey only consider one source domain <mathjax>$\mathcal{D}_S$</mathjax>, one target domain <mathjax>$\mathcal{D}_T$</mathjax> and usually <mathjax>$0 \le N_T \ll N_S$</mathjax>.</p>
<h4 id="unified-definition">Unified definition</h4>
<p><strong>Transfer Learning</strong>: Given a source domain <mathjax>$\mathcal{D}_S$</mathjax> and learning task <mathjax>$\mathcal{T}_S$</mathjax>, a target domain <mathjax>$\mathcal{D}_T$</mathjax> and learning task <mathjax>$\mathcal{T}_T$</mathjax>, transfer learning aims to help improve the learning of the target predictive function <mathjax>$f_T(\cdot)$</mathjax> in <mathjax>$\mathcal{D}_T$</mathjax> using the knowledge in <mathjax>$\mathcal{D}_S$</mathjax> and <mathjax>$\mathcal{T}_S$</mathjax>,where <mathjax>$\mathcal{D}_S \neq \mathcal{D}_T$</mathjax>, or <mathjax>$\mathcal{T}_S \neq \mathcal{T}_T$</mathjax>.</p>
<h3 id="a-categorization-of-transfer-learning-techniques">A Categorization of Transfer Learning Techniques</h3>
<p>Three main research issues:</p>
<ol>
<li>what to transfer</li>
<li>how to transfer</li>
<li>when to transfer</li>
</ol>
<p>Three subsettings:</p>
<ol>
<li>inductive transfer learning</li>
<li>transductive transfer learning</li>
<li>unsupervised transfer learning</li>
</ol>
<h5 id="inductive-transfer-learning">inductive transfer learning</h5>
<p>The target task is different from the source task, no matter when the source and target domains are the same or not.</p>
<p>Two cases:<br />
1. A lot of labeled data in the source domain are available.<br />
2. No labeled data in the source domain are available.</p>
<h5 id="transductive-transfer-learning">transductive transfer learning</h5>
<p>The source and target tasks are the same, while the source and target domains are different.</p>
<p>Two cases:<br />
1. <mathjax>$\mathcal{X}_S \neq \mathcal{X}_T$</mathjax>.<br />
2. <mathjax>$\mathcal{X}_S = \mathcal{X}_T$</mathjax> but <mathjax>$P(X_S) \neq P(X_T)$</mathjax>.</p>
<h5 id="unsupervised-transfer-learning">unsupervised transfer learning</h5>
<p>no labeled data available in both source and target domains in training.</p>
<h2 id="inductive-transfer-learning_1">Inductive transfer learning</h2>
<p><strong>Inductive transfer learning</strong>: Given a source domain <mathjax>$\mathcal{D}_S$</mathjax> and a learning task <mathjax>$\mathcal{T}_S$</mathjax>, a target domain <mathjax>$\mathcal{D}_T$</mathjax> and a learning task <mathjax>$\mathcal{T}_T$</mathjax>, inductive transfer learning aims to help improve the learning of the target predictive function <mathjax>$f_T(\cdot)$</mathjax> in <mathjax>$\mathcal{D}_T$</mathjax> using the knowledge in <mathjax>$\mathcal{D}_S$</mathjax> and <mathjax>$\mathcal{T}_S$</mathjax> where <mathjax>$\mathcal{T}_S \neq \mathcal{T}_T$</mathjax>.</p>
<h3 id="transferring-knowledge-of-instances">Transferring Knowledge of Instances</h3>
<p><strong>a boosting algorithm</strong>: <em>TrAdaBoost</em></p>
<h3 id="transferring-knowledge-of-feature-representations">Transferring Knowledge of Feature Representations</h3>
<p>Aims at finding “good” feature representations to minimize domain divergence and classi- fication or regression model error. </p>
<p>Similar to <em>common feature learning</em> in the field of multitask learning</p>
<h4 id="supervised-feature-construction">Supervised Feature Construction</h4>
<p>In the <em>inductive transfer learning</em> setting, the common features can be learned by solving an optimization problem, given as follows:</p>
<p><mathjax>$$ argmin_{A, U} \sum_{t \in \{T, S\}} \sum_{i=1}^{n_t} L(y_{t_i}, \langle a_t, U^T x_{t_i}\rangle) + \gamma ||A||^2_{2, 1}$$</mathjax> </p>
<p><mathjax>$$s.t. U \in \mathbf{O}^d .$$</mathjax></p>
<h4 id="unsupervised-feature-construction">Unsupervised Feature Construction</h4>
<p>Sparse coding is an unsupervised feature construction method for learning <em>higher level</em> features for transfer learning. It consists of two steps.</p>
<p>In the first step, higher level basis vectors <mathjax>$b = \{b_1, b_2, \dots, b_s\}$</mathjax> are learned from</p>
<p><mathjax>$$ min_{a, b}\sum_i ||x_{S_i} - \sum_j a_{S_i}^j b_j||^2_2 + \beta ||a_{S_i}||_1$$</mathjax></p>
<p><mathjax>$$s.t. ||b_j||_2 \le 1 , \forall j \in 1, \dots, s. $$</mathjax></p>
<p>In the second step, <em>higher level</em> features on the target-domain data will be learned based on the basis vectors <mathjax>$b$</mathjax>:</p>
<p><mathjax>$$a_{T_i}^* = argmin_{a_{T_i}} ||x_{T_i} - \sum_j a_{T_i}^j b_j||^2_2 + \beta||a_{T_i}||_1$$</mathjax></p>
<h3 id="transferring-knowledge-of-parameters">Transferring Knowledge of Parameters</h3>
<p>Most approaches are designed to work under multitask learning. But they can be easily modified for transfer learning. Intuitively, we may assign a larger weight to the loss function of the target domain to achieve better performance in the target domain.</p>
<p><strong>Algorithms</strong>: </p>
<ul>
<li>MT-IVM (Lawrence and Platt)</li>
<li>SVMs for multitask learning (Evgeniou and Pontil)</li>
<li>A locally weighted ensemble learning framework (Gao et al)</li>
</ul>
<h3 id="transferring-relational-knowledge">Transferring Relational Knowledge</h3>
<p>It deals with transfer learning problems in relational domains. It does not assume that the data drawn from each domain be independent and identically distributed (i.i.d.).</p>
<p><em>Statistical relational learning techniques</em> are proposed to solve these problems.</p>
<p><strong>Algorithms</strong>: </p>
<ul>
<li>TAMAR (Mihalkova et al)</li>
</ul>
<h2 id="transductive-transfer-learning_1">Transductive transfer learning</h2>
<h3 id="transferring-the-knowledge-of-instances">Transferring the Knowledge of Instances</h3>
<p>We want to minimize<br />
<mathjax>$$ \theta^* = argmin_{\theta \in \Theta} \sum_{(x, y)\in D_S} \frac{P(D_T)}{P(D_S)}P(D_S)l(x,y,\theta) \\
\approx argmin_{\theta \in \Theta} \sum_{i=1}^{n_S}\frac{P_T(x_{T_i}, y_{T_i})}{P_S(x_{S_i}, y_{S_i})} l(x_{S_i}, y_{S_i}, \theta).$$</mathjax><br />
and<br />
<mathjax>$$\frac{P_T(x_{T_i}, y_{T_i})}{P_S(x_{S_i}, y_{S_i})}= \frac{P(x_{S_i})}{P(x_{T_i})}$$</mathjax></p>
<p>Algorithms to estimate <mathjax>$\frac{P(x_{S_i})}{P(x_{T_i})}$</mathjax>:</p>
<ul>
<li>kernel-mean matching(KMM）</li>
<li>Kullback-Leibler Importance Estimation Procedure(KLIEP)</li>
</ul>
<h3 id="transferring-knowledge-of-feature-representations_1">Transferring Knowledge of Feature Representations</h3>
<p>Structural correspondence learning (SCL) algorithm：<br />
 1. Define a set of <em>pivot</em> features from both domains<br />
 2. Remove these <em>pivot</em> features from the data and treats each <em>pivot</em> feature as a new label vector. Solve <mathjax>$m$</mathjax> classification problem:<br />
<mathjax>$$f_1(x)= sgn(w_l^T \cdot x), l=1, \dots, m.$$</mathjax><br />
 3. Use SVD on matrix <mathjax>$W$</mathjax>: <mathjax>$W = UDV^T$</mathjax>, and <mathjax>$\theta = U^T_{[1:h,:]}$</mathjax>(h is the number of the shared features)is the matrix (linear mapping) whose rows are the top left singular vectors of W.<br />
 4. Use the augmented feature vector to build models.</p>
<p>Disadvantages: How to select the pivot features is difficult and domain dependent.</p>
<p>MI-SCL: Use Mutual Information (MI) to choose the pivot features instead of using more heuristic criteria.</p>
<p>Other algorithms:</p>
<ul>
<li>coclustering-based algorithm: propagate the label information across different domains.</li>
<li>bridged refinement</li>
<li>spectral classification framework for cross-domain transfer learning problem</li>
<li>a cross-domain text classification algorithm that extended the traditional prob- abilistic latent semantic analysis (PLSA) algorithm</li>
</ul>
<p>Transfer learning via dimensionality reduction:<br />
Maximum Mean Discrepancy Embedding(MMDE) can learn a low-dimensional space to reduce the difference of distributions between different domains. Transfer Component Analysis (TCA) overcomes the drawback of computational burden.</p>
<h2 id="unsupervised-transfer-learning_1">Unsupervised Transfer Learning</h2>
<p>Little research works on this setting. <em>Self-taught clustering</em> (STC) and transferred discriminative analysis (TDA) algorithms are proposed to transfer clustering and transfer dimensionality reduction problems, respectively.</p>
<h3 id="transferring-knowledge-of-feature-representations_2">Transferring Knowledge of Feature Representations</h3>
<p><em>Self-taught clustering</em> is an instance of <em>unsupervised transfer learning</em>, which aims at clustering a small collection of unlabeled data in the target domain with the help of a large amount of unlabeled data in the source domain.</p>
<p>STC: tries to learn a common feature space across domains.</p>
<p><em>TDA</em> algorithm: solves the transfer dimensionality reduction problem. First applies clustering methods to generate pseudoclass labels for the target unlabeled data, then applies dimensionality reduction methods to the target data and labeled source data to reduce the dimensions. Run iteratively.</p>
<h2 id="transfer-bounds-and-negative-transfer">Transfer Bounds and Negative Transfer</h2>
<p>Conditional Kolmogorov complexity is used to measure relatedness between tasks and transfer the “right” amount of information. </p>
<p>A novel graph-based method is used for knowledge transfer.</p>
<p>How to avoid negative transfer is a very important issue. If two tasks are too dissimilar, then brute-force transfer may hurt the performance.</p>
<h2 id="applications-of-transfer-learning">Applications of Transfer Learning</h2>
<p>Datasets:<br />
- text mining data sets<br />
- Email spam-filtering data set<br />
- WiFi localization over time periods data set<br />
- Sentiment classification data set</p>
<p>Toolboxes:<br />
a MATLAB toolkit for transfer learning. http://multitask.cs.berkeley.edu/</p>
<p>Other applications:<br />
In sequential machine learning</p>
<h2 id="conclusion">Conclusion</h2>
<p>This survey reviewed several current trends of transfer learning. Three different settings of Transfer Learning: inductive transfer learning, transductive transfer learning, and unsupervised transfer learning. </p>
<p>Approaches can be classified into four contexts based on “what to transfer” in learning.</p>
<p>Future research issues:<br />
- how to avoid negative transfer<br />
- how to make sure that no negative transfer happens<br />
- when an entire domain cannot be used for transfer learning and whether we can still transfer part of the domain for useful learning in the target domain.</p>
<p>Most transfer learning algorithms assumed that the feature spaces between the source and target domains are the same. However, we may wish to transfer knowledge across domains or tasks that have different feature spaces, and transfer from multiple such source domains. This type is <em>heterogeneous transfer learning</em>.</p>
<p>Use transfer learning to solve other challenging applications.</p>
          <hr/>
          <div class="comment"></div>
          <script src="/valine/myvaline.js" ></script>
        </div>
      </div>
      <div class="mdl-cell mdl-cell--3-col mdl-cell--hide-tablet mdl-cell--hide-phone sidebar">
        <div class="article">
          <div class="mdl-card mdl-shadow--3dp sidebar-card">
            <div class="mdl-card__actions sidebar-title">页面信息</div>
            <div class="mdl-card__supporting-text">
              标签: <a href="/search.html?q=Notes"><span class="label">Notes</span></a><br/>
              创建时间: 2018.06.02<br/>
              上次修改: 2018.07.08<br/>
              字数统计: 11756 字 / 约 47 分钟
            </div>
          </div>
          <br/>
          
<div class="mdl-card mdl-shadow--3dp sidebar-card">
  <div class="mdl-card__actions sidebar-title">目录</div>
  <div class="mdl-card__supporting-text">
    <div class="toc">
<ul>
<li><a href="#a-note-of-a-survey-on-transfer-learning">A note of A Survey on Transfer Learning</a><ul>
<li><a href="#introduction">Introduction</a><ul>
<li><a href="#applied-range">Applied range</a></li>
<li><a href="#examples">Examples</a></li>
</ul>
</li>
<li><a href="#overview">Overview</a><ul>
<li><a href="#brief-history">Brief History</a><ul>
<li><a href="#history">History</a></li>
<li><a href="#transfer-learning">Transfer Learning</a></li>
</ul>
</li>
<li><a href="#notations-and-definitions">Notations and Definitions</a><ul>
<li><a href="#definitions-of-domain-and-task">Definitions of &ldquo;Domain&rdquo; and &ldquo;Task&rdquo;</a></li>
<li><a href="#unified-definition">Unified definition</a></li>
</ul>
</li>
<li><a href="#a-categorization-of-transfer-learning-techniques">A Categorization of Transfer Learning Techniques</a><ul>
<li><a href="#inductive-transfer-learning">inductive transfer learning</a></li>
<li><a href="#transductive-transfer-learning">transductive transfer learning</a></li>
<li><a href="#unsupervised-transfer-learning">unsupervised transfer learning</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#inductive-transfer-learning_1">Inductive transfer learning</a><ul>
<li><a href="#transferring-knowledge-of-instances">Transferring Knowledge of Instances</a></li>
<li><a href="#transferring-knowledge-of-feature-representations">Transferring Knowledge of Feature Representations</a><ul>
<li><a href="#supervised-feature-construction">Supervised Feature Construction</a></li>
<li><a href="#unsupervised-feature-construction">Unsupervised Feature Construction</a></li>
</ul>
</li>
<li><a href="#transferring-knowledge-of-parameters">Transferring Knowledge of Parameters</a></li>
<li><a href="#transferring-relational-knowledge">Transferring Relational Knowledge</a></li>
</ul>
</li>
<li><a href="#transductive-transfer-learning_1">Transductive transfer learning</a><ul>
<li><a href="#transferring-the-knowledge-of-instances">Transferring the Knowledge of Instances</a></li>
<li><a href="#transferring-knowledge-of-feature-representations_1">Transferring Knowledge of Feature Representations</a></li>
</ul>
</li>
<li><a href="#unsupervised-transfer-learning_1">Unsupervised Transfer Learning</a><ul>
<li><a href="#transferring-knowledge-of-feature-representations_2">Transferring Knowledge of Feature Representations</a></li>
</ul>
</li>
<li><a href="#transfer-bounds-and-negative-transfer">Transfer Bounds and Negative Transfer</a></li>
<li><a href="#applications-of-transfer-learning">Applications of Transfer Learning</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
</li>
</ul>
</div>
  </div>
</div>
<br/>

          <div class="mdl-card mdl-shadow--3dp sidebar-card">
            <div class="mdl-card__actions sidebar-title">数学公式渲染</div>
            <div class="mdl-card__supporting-text">
              <form name="mathopt">
                <label class="mdl-radio mdl-js-radio mdl-js-ripple-effect" for="option-1">
                  <input type="radio" id="option-1" class="mdl-radio__button" name="sel" value="mathjax">
                  <span class="mdl-radio__label">MathJax (推荐)</span>
                </label><br/>
                <div id="tip-1">
                  <label class="mdl-radio mdl-js-radio mdl-js-ripple-effect" for="option-2">
                    <input type="radio" id="option-2" class="mdl-radio__button" name="sel" value="katex">
                    <span class="mdl-radio__label">KaTeX</span>
                  </label>
                </div>
                <div id="tip-2">
                  <label class="mdl-radio mdl-js-radio mdl-js-ripple-effect" for="option-3">
                    <input type="radio" id="option-3" class="mdl-radio__button" name="sel" value="katex&mathjax">
                    <span class="mdl-radio__label">Mixed</span>
                  </label>
                </div>
              </form>
            </div>
          </div>
          <div class="mdl-tooltip" data-mdl-for="tip-1">KaTeX 渲染效率很高，但是目前 KaTeX 容错性不强，因此使用 KaTeX 时可能会存在一些数学公式无法渲染的情况</div>
          <div class="mdl-tooltip" data-mdl-for="tip-2">先使用 KaTeX 渲染，再使用 MathJax 渲染</div>
          <br/>
          <div class="mdl-card mdl-shadow--3dp sidebar-card">
            <div class="mdl-card__actions sidebar-title">A Sentence</div>
            <div class="mdl-card__supporting-text">
              <strong>
                <script src="text.js">
            </script>
              </strong>
            </div>
          </div>
        </div>
      </div>
    </div>
    <footer class="mdl-mega-footer">
      <div class="mdl-mega-footer__middle-section">
        <div class="mdl-mega-footer__drop-down-section">
          <input class="mdl-mega-footer__heading-checkbox" type="checkbox" checked>
          <h1 class="mdl-mega-footer__heading">LAN-QING.SITE</h1> lan-qing
        </div>
        <div class="mdl-mega-footer__drop-down-section">
          <input class="mdl-mega-footer__heading-checkbox" type="checkbox" checked>
          <h1 class="mdl-mega-footer__heading">POWERED BY</h1>
          <ul class="mdl-mega-footer__link-list">
            <li><a href="http://pythonhosted.org/Markdown/">Python Markdown</a></li>
            <li><a href="http://getmdl.io/">Material Design Lite</a></li>
            <li><a href="http://www.tipue.com/search/">Tipuesearch</a></li>
            <li><a href="http://www.mathjax.org/">MathJax</a> & <a href="http://khan.github.io/KaTeX/">KaTeX</a></li>
            <li><a href="https://github.com/imsun/gitment">Gitment</a></li>
          </ul>
        </div>
        <!--<div class="mdl-mega-footer__drop-down-section">-->
          <!--<input class="mdl-mega-footer__heading-checkbox" type="checkbox" checked>-->
          <!--<h1 class="mdl-mega-footer__heading">友情链接</h1>-->
          <!--<ul class="mdl-mega-footer__link-list">-->
            <!--<li><a href="http://ruanx.pw/">ruanxingzhi</a></li>-->
            <!--<li><a href="https://blog.xehoth.cc/">xehoth</a></li>-->
            <!--<li><a href="http://hjwjbsr.is-programmer.com/">HJWJBSR</a></li>-->
            <!--<li><a href="http://www.micdz.cn/">MicDZ</a></li>-->
            <!--<li><a href="http://blog.linyxus.xyz/">Linyxus</a></li>-->
            <!--<li><a href="http://memset0.cf/">memset0</a></li>-->
          <!--</ul>-->
        <!--</div>-->
      </div>
      <div class="mdl-mega-footer__bottom-section">Theme based on <a href="https://getmdl.io/">MDL</a> | Copyright © 2018 lan-qing. All rights reserved.</div>
    </footer>
  </main>
</div>
</body>
</html>
